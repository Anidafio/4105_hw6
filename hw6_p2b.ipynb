{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anidafio/4105_hw6/blob/main/hw6_p2b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWgkc9WHw9zw",
        "outputId": "f1cdfb95-98e4-433d-a3a0-dc1baffa31d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 42070724.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                             (0.5, 0.5, 0.5))\n",
        "    ]))\n",
        "\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                             (0.5, 0.5, 0.5))\n",
        "    ]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD58A7Acz1fu",
        "outputId": "6374ab56-a919-41c8-e9e6-160fbe6eed82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.766799\n",
            "Epoch: 1, Loss: 1.710466\n",
            "Epoch: 2, Loss: 1.580109\n",
            "Epoch: 3, Loss: 1.534468\n",
            "Epoch: 4, Loss: 1.519469\n",
            "Epoch: 5, Loss: 1.479033\n",
            "Epoch: 6, Loss: 1.450589\n",
            "Epoch: 7, Loss: 1.417462\n",
            "Epoch: 8, Loss: 1.372842\n",
            "Epoch: 9, Loss: 1.296277\n",
            "Epoch: 10, Loss: 1.288098\n",
            "Epoch: 11, Loss: 1.235039\n",
            "Epoch: 12, Loss: 1.220152\n",
            "Epoch: 13, Loss: 1.274926\n",
            "Epoch: 14, Loss: 1.129364\n",
            "Epoch: 15, Loss: 1.086619\n",
            "Epoch: 16, Loss: 1.253833\n",
            "Epoch: 17, Loss: 1.075759\n",
            "Epoch: 18, Loss: 1.067894\n",
            "Epoch: 19, Loss: 0.998716\n",
            "Epoch: 20, Loss: 0.911017\n",
            "Epoch: 21, Loss: 0.982696\n",
            "Epoch: 22, Loss: 0.934062\n",
            "Epoch: 23, Loss: 0.999917\n",
            "Epoch: 24, Loss: 0.984625\n",
            "Epoch: 25, Loss: 0.918442\n",
            "Epoch: 26, Loss: 0.967475\n",
            "Epoch: 27, Loss: 0.844298\n",
            "Epoch: 28, Loss: 0.791569\n",
            "Epoch: 29, Loss: 0.943544\n",
            "Epoch: 30, Loss: 0.799151\n",
            "Epoch: 31, Loss: 0.803558\n",
            "Epoch: 32, Loss: 0.763051\n",
            "Epoch: 33, Loss: 0.645461\n",
            "Epoch: 34, Loss: 0.677197\n",
            "Epoch: 35, Loss: 0.710853\n",
            "Epoch: 36, Loss: 0.717749\n",
            "Epoch: 37, Loss: 0.608863\n",
            "Epoch: 38, Loss: 0.645732\n",
            "Epoch: 39, Loss: 0.686290\n",
            "Epoch: 40, Loss: 0.577413\n",
            "Epoch: 41, Loss: 0.613328\n",
            "Epoch: 42, Loss: 0.574375\n",
            "Epoch: 43, Loss: 0.548272\n",
            "Epoch: 44, Loss: 0.609397\n",
            "Epoch: 45, Loss: 0.632804\n",
            "Epoch: 46, Loss: 0.539742\n",
            "Epoch: 47, Loss: 0.575742\n",
            "Epoch: 48, Loss: 0.621561\n",
            "Epoch: 49, Loss: 0.609728\n",
            "Epoch: 50, Loss: 0.476269\n",
            "Epoch: 51, Loss: 0.646183\n",
            "Epoch: 52, Loss: 0.605537\n",
            "Epoch: 53, Loss: 0.549261\n",
            "Epoch: 54, Loss: 0.616020\n",
            "Epoch: 55, Loss: 0.526453\n",
            "Epoch: 56, Loss: 0.611505\n",
            "Epoch: 57, Loss: 0.590406\n",
            "Epoch: 58, Loss: 0.423240\n",
            "Epoch: 59, Loss: 0.480589\n",
            "Epoch: 60, Loss: 0.447921\n",
            "Epoch: 61, Loss: 0.598815\n",
            "Epoch: 62, Loss: 0.545649\n",
            "Epoch: 63, Loss: 0.402626\n",
            "Epoch: 64, Loss: 0.585924\n",
            "Epoch: 65, Loss: 0.561152\n",
            "Epoch: 66, Loss: 0.501207\n",
            "Epoch: 67, Loss: 0.539542\n",
            "Epoch: 68, Loss: 0.410705\n",
            "Epoch: 69, Loss: 0.509828\n",
            "Epoch: 70, Loss: 0.564343\n",
            "Epoch: 71, Loss: 0.515068\n",
            "Epoch: 72, Loss: 0.421337\n",
            "Epoch: 73, Loss: 0.571026\n",
            "Epoch: 74, Loss: 0.581981\n",
            "Epoch: 75, Loss: 0.536993\n",
            "Epoch: 76, Loss: 0.447285\n",
            "Epoch: 77, Loss: 0.384160\n",
            "Epoch: 78, Loss: 0.511322\n",
            "Epoch: 79, Loss: 0.646146\n",
            "Epoch: 80, Loss: 0.511412\n",
            "Epoch: 81, Loss: 0.522136\n",
            "Epoch: 82, Loss: 0.387022\n",
            "Epoch: 83, Loss: 0.406277\n",
            "Epoch: 84, Loss: 0.390451\n",
            "Epoch: 85, Loss: 0.439940\n",
            "Epoch: 86, Loss: 0.469016\n",
            "Epoch: 87, Loss: 0.465797\n",
            "Epoch: 88, Loss: 0.488869\n",
            "Epoch: 89, Loss: 0.542526\n",
            "Epoch: 90, Loss: 0.467270\n",
            "Epoch: 91, Loss: 0.482208\n",
            "Epoch: 92, Loss: 0.620047\n",
            "Epoch: 93, Loss: 0.466645\n",
            "Epoch: 94, Loss: 0.480364\n",
            "Epoch: 95, Loss: 0.350717\n",
            "Epoch: 96, Loss: 0.502495\n",
            "Epoch: 97, Loss: 0.432036\n",
            "Epoch: 98, Loss: 0.619008\n",
            "Epoch: 99, Loss: 0.476396\n",
            "Epoch: 100, Loss: 0.511931\n",
            "Epoch: 101, Loss: 0.487626\n",
            "Epoch: 102, Loss: 0.373776\n",
            "Epoch: 103, Loss: 0.456547\n",
            "Epoch: 104, Loss: 0.533251\n",
            "Epoch: 105, Loss: 0.471186\n",
            "Epoch: 106, Loss: 0.402301\n",
            "Epoch: 107, Loss: 0.584559\n",
            "Epoch: 108, Loss: 0.541914\n",
            "Epoch: 109, Loss: 0.417127\n",
            "Epoch: 110, Loss: 0.483745\n",
            "Epoch: 111, Loss: 0.450729\n",
            "Epoch: 112, Loss: 0.470960\n",
            "Epoch: 113, Loss: 0.551533\n",
            "Epoch: 114, Loss: 0.413143\n",
            "Epoch: 115, Loss: 0.428548\n",
            "Epoch: 116, Loss: 0.391210\n",
            "Epoch: 117, Loss: 0.486961\n",
            "Epoch: 118, Loss: 0.373666\n",
            "Epoch: 119, Loss: 0.456743\n",
            "Epoch: 120, Loss: 0.497900\n",
            "Epoch: 121, Loss: 0.458599\n",
            "Epoch: 122, Loss: 0.518250\n",
            "Epoch: 123, Loss: 0.433279\n",
            "Epoch: 124, Loss: 0.523498\n",
            "Epoch: 125, Loss: 0.322432\n",
            "Epoch: 126, Loss: 0.512305\n",
            "Epoch: 127, Loss: 0.421406\n",
            "Epoch: 128, Loss: 0.560840\n",
            "Epoch: 129, Loss: 0.532010\n",
            "Epoch: 130, Loss: 0.499226\n",
            "Epoch: 131, Loss: 0.478740\n",
            "Epoch: 132, Loss: 0.349089\n",
            "Epoch: 133, Loss: 0.476543\n",
            "Epoch: 134, Loss: 0.484633\n",
            "Epoch: 135, Loss: 0.338141\n",
            "Epoch: 136, Loss: 0.379785\n",
            "Epoch: 137, Loss: 0.482513\n",
            "Epoch: 138, Loss: 0.315792\n",
            "Epoch: 139, Loss: 0.378825\n",
            "Epoch: 140, Loss: 0.361160\n",
            "Epoch: 141, Loss: 0.477808\n",
            "Epoch: 142, Loss: 0.546470\n",
            "Epoch: 143, Loss: 0.478309\n",
            "Epoch: 144, Loss: 0.469876\n",
            "Epoch: 145, Loss: 0.388871\n",
            "Epoch: 146, Loss: 0.388395\n",
            "Epoch: 147, Loss: 0.381816\n",
            "Epoch: 148, Loss: 0.480836\n",
            "Epoch: 149, Loss: 0.481476\n",
            "Epoch: 150, Loss: 0.419917\n",
            "Epoch: 151, Loss: 0.388953\n",
            "Epoch: 152, Loss: 0.506526\n",
            "Epoch: 153, Loss: 0.478475\n",
            "Epoch: 154, Loss: 0.458719\n",
            "Epoch: 155, Loss: 0.443962\n",
            "Epoch: 156, Loss: 0.316647\n",
            "Epoch: 157, Loss: 0.484925\n",
            "Epoch: 158, Loss: 0.600727\n",
            "Epoch: 159, Loss: 0.542074\n",
            "Epoch: 160, Loss: 0.452993\n",
            "Epoch: 161, Loss: 0.580929\n",
            "Epoch: 162, Loss: 0.330858\n",
            "Epoch: 163, Loss: 0.437829\n",
            "Epoch: 164, Loss: 0.355683\n",
            "Epoch: 165, Loss: 0.446414\n",
            "Epoch: 166, Loss: 0.615776\n",
            "Epoch: 167, Loss: 0.377678\n",
            "Epoch: 168, Loss: 0.420250\n",
            "Epoch: 169, Loss: 0.402833\n",
            "Epoch: 170, Loss: 0.494503\n",
            "Epoch: 171, Loss: 0.460866\n",
            "Epoch: 172, Loss: 0.547934\n",
            "Epoch: 173, Loss: 0.548036\n",
            "Epoch: 174, Loss: 0.462905\n",
            "Epoch: 175, Loss: 0.367278\n",
            "Epoch: 176, Loss: 0.475154\n",
            "Epoch: 177, Loss: 0.353799\n",
            "Epoch: 178, Loss: 0.417471\n",
            "Epoch: 179, Loss: 0.605810\n",
            "Epoch: 180, Loss: 0.445345\n",
            "Epoch: 181, Loss: 0.418320\n",
            "Epoch: 182, Loss: 0.412673\n",
            "Epoch: 183, Loss: 0.474879\n",
            "Epoch: 184, Loss: 0.569683\n",
            "Epoch: 185, Loss: 0.464921\n",
            "Epoch: 186, Loss: 0.382131\n",
            "Epoch: 187, Loss: 0.358349\n",
            "Epoch: 188, Loss: 0.492338\n",
            "Epoch: 189, Loss: 0.488021\n",
            "Epoch: 190, Loss: 0.384679\n",
            "Epoch: 191, Loss: 0.396833\n",
            "Epoch: 192, Loss: 0.444135\n",
            "Epoch: 193, Loss: 0.413184\n",
            "Epoch: 194, Loss: 0.402314\n",
            "Epoch: 195, Loss: 0.481006\n",
            "Epoch: 196, Loss: 0.441286\n",
            "Epoch: 197, Loss: 0.512755\n",
            "Epoch: 198, Loss: 0.436659\n",
            "Epoch: 199, Loss: 0.487928\n",
            "Epoch: 200, Loss: 0.330732\n",
            "Epoch: 201, Loss: 0.403310\n",
            "Epoch: 202, Loss: 0.502566\n",
            "Epoch: 203, Loss: 0.454717\n",
            "Epoch: 204, Loss: 0.372864\n",
            "Epoch: 205, Loss: 0.362261\n",
            "Epoch: 206, Loss: 0.348680\n",
            "Epoch: 207, Loss: 0.373028\n",
            "Epoch: 208, Loss: 0.417570\n",
            "Epoch: 209, Loss: 0.303191\n",
            "Epoch: 210, Loss: 0.358705\n",
            "Epoch: 211, Loss: 0.413815\n",
            "Epoch: 212, Loss: 0.558134\n",
            "Epoch: 213, Loss: 0.459480\n",
            "Epoch: 214, Loss: 0.497583\n",
            "Epoch: 215, Loss: 0.451145\n",
            "Epoch: 216, Loss: 0.376887\n",
            "Epoch: 217, Loss: 0.362113\n",
            "Epoch: 218, Loss: 0.657983\n",
            "Epoch: 219, Loss: 0.484278\n",
            "Epoch: 220, Loss: 0.508978\n",
            "Epoch: 221, Loss: 0.408661\n",
            "Epoch: 222, Loss: 0.426609\n",
            "Epoch: 223, Loss: 0.515971\n",
            "Epoch: 224, Loss: 0.372574\n",
            "Epoch: 225, Loss: 0.547910\n",
            "Epoch: 226, Loss: 0.474226\n",
            "Epoch: 227, Loss: 0.581812\n",
            "Epoch: 228, Loss: 0.472690\n",
            "Epoch: 229, Loss: 0.403401\n",
            "Epoch: 230, Loss: 0.451121\n",
            "Epoch: 231, Loss: 0.378465\n",
            "Epoch: 232, Loss: 0.474695\n",
            "Epoch: 233, Loss: 0.387386\n",
            "Epoch: 234, Loss: 0.577786\n",
            "Epoch: 235, Loss: 0.347227\n",
            "Epoch: 236, Loss: 0.415605\n",
            "Epoch: 237, Loss: 0.524038\n",
            "Epoch: 238, Loss: 0.506230\n",
            "Epoch: 239, Loss: 0.512005\n",
            "Epoch: 240, Loss: 0.545504\n",
            "Epoch: 241, Loss: 0.455060\n",
            "Epoch: 242, Loss: 0.358716\n",
            "Epoch: 243, Loss: 0.430735\n",
            "Epoch: 244, Loss: 0.522240\n",
            "Epoch: 245, Loss: 0.532632\n",
            "Epoch: 246, Loss: 0.467028\n",
            "Epoch: 247, Loss: 0.499424\n",
            "Epoch: 248, Loss: 0.422097\n",
            "Epoch: 249, Loss: 0.480236\n",
            "Epoch: 250, Loss: 0.506510\n",
            "Epoch: 251, Loss: 0.392949\n",
            "Epoch: 252, Loss: 0.413314\n",
            "Epoch: 253, Loss: 0.553179\n",
            "Epoch: 254, Loss: 0.567770\n",
            "Epoch: 255, Loss: 0.447169\n",
            "Epoch: 256, Loss: 0.341546\n",
            "Epoch: 257, Loss: 0.589905\n",
            "Epoch: 258, Loss: 0.570874\n",
            "Epoch: 259, Loss: 0.540898\n",
            "Epoch: 260, Loss: 0.458394\n",
            "Epoch: 261, Loss: 0.380907\n",
            "Epoch: 262, Loss: 0.529703\n",
            "Epoch: 263, Loss: 0.470599\n",
            "Epoch: 264, Loss: 0.470610\n",
            "Epoch: 265, Loss: 0.453680\n",
            "Epoch: 266, Loss: 0.412585\n",
            "Epoch: 267, Loss: 0.320469\n",
            "Epoch: 268, Loss: 0.303175\n",
            "Epoch: 269, Loss: 0.472549\n",
            "Epoch: 270, Loss: 0.395344\n",
            "Epoch: 271, Loss: 0.401792\n",
            "Epoch: 272, Loss: 0.404110\n",
            "Epoch: 273, Loss: 0.511305\n",
            "Epoch: 274, Loss: 0.328962\n",
            "Epoch: 275, Loss: 0.519150\n",
            "Epoch: 276, Loss: 0.557800\n",
            "Epoch: 277, Loss: 0.449611\n",
            "Epoch: 278, Loss: 0.475363\n",
            "Epoch: 279, Loss: 0.376385\n",
            "Epoch: 280, Loss: 0.413590\n",
            "Epoch: 281, Loss: 0.547903\n",
            "Epoch: 282, Loss: 0.496950\n",
            "Epoch: 283, Loss: 0.470087\n",
            "Epoch: 284, Loss: 0.382907\n",
            "Epoch: 285, Loss: 0.450333\n",
            "Epoch: 286, Loss: 0.700190\n",
            "Epoch: 287, Loss: 0.399961\n",
            "Epoch: 288, Loss: 0.510109\n",
            "Epoch: 289, Loss: 0.376396\n",
            "Epoch: 290, Loss: 0.455436\n",
            "Epoch: 291, Loss: 0.514861\n",
            "Epoch: 292, Loss: 0.518549\n",
            "Epoch: 293, Loss: 0.519710\n",
            "Epoch: 294, Loss: 0.347193\n",
            "Epoch: 295, Loss: 0.435413\n",
            "Epoch: 296, Loss: 0.543378\n",
            "Epoch: 297, Loss: 0.502875\n",
            "Epoch: 298, Loss: 0.349793\n",
            "Epoch: 299, Loss: 0.537380\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=100, shuffle=False)\n",
        "\n",
        "seq_model = nn.Sequential(\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "optimizer = optim.Adam(seq_model.parameters(), lr=1e-3)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mr-ATDyAgeqb",
        "outputId": "a4ab7ea7-6ba1-41dc-f021-fd18b20f2e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.862980\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GZ18mbiLgzZR",
        "outputId": "538176a2-0b6c-4ef9-ca09-0b0f384a1cab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.462100\n"
          ]
        }
      ],
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8RdGONxbKBxz4DTtxhBy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}